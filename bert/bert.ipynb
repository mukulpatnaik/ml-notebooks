{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 13:47:28.654543: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'squad_train.tf_record'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Load train and test data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m train_examples \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 15\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39msquad_train.tf_record\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     16\u001b[0m   \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     example \u001b[39m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: line[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: line[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mend_position\u001b[39m\u001b[39m'\u001b[39m: line[\u001b[39m'\u001b[39m\u001b[39mend_position\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     23\u001b[0m     }\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'squad_train.tf_record'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set hyperparameters\n",
    "max_seq_length = 384\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "num_train_epochs = 2.0\n",
    "\n",
    "# Load train and test data\n",
    "train_examples = []\n",
    "with open('squad_train.tf_record') as f:\n",
    "  for line in f:\n",
    "    example = {\n",
    "        'input_ids': line['input_ids'],\n",
    "        'attention_mask': line['attention_mask'],\n",
    "        'token_type_ids': line['token_type_ids'],\n",
    "        'start_position': line['start_position'],\n",
    "        'end_position': line['end_position'],\n",
    "    }\n",
    "    train_examples.append(example)\n",
    "num_train_steps = int(len(train_examples) / batch_size * num_train_epochs)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_examples)\n",
    "train_dataset = train_dataset.shuffle(100).batch(batch_size).prefetch(10)\n",
    "test_examples = []\n",
    "with open('squad_test.tf_record') as f:\n",
    "  for line in f:\n",
    "    example = {\n",
    "        'input_ids': line['input_ids'],\n",
    "        'attention_mask': line['attention_mask'],\n",
    "        'token_type_ids': line['token_type_ids'],\n",
    "    }\n",
    "    test_examples.append(example)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_examples)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(10)\n",
    "\n",
    "# Build BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('uncased_L-12_H-768_A-12')\n",
    "model = TFBertForQuestionAnswering.from_pretrained('uncased_L-12_H-768_A-12')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Fine-tune BERT on train set\n",
    "for epoch in range(num_train_epochs):\n",
    "  for step, batch in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(batch['input_ids'], attention_mask=batch['attention_mask'], token_type_ids=batch['token_type_ids'])\n",
    "      start_logits, end_logits = logits\n",
    "      start_loss = loss(batch['start_position'], start_logits)\n",
    "      end_loss = loss(batch['end_position'], end_logits)\n",
    "      total_loss = tf.reduce_mean(start_loss + end_loss)\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    if step % 100 == 0:\n",
    "      print('Epoch {} Step {} Loss {:.4f}'.format(epoch + 1, step, total_loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import random\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load SQuAD dataset\n",
    "with open('train-v1.1.json', 'r') as f:\n",
    "  squad_dataset = json.load(f)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "random.seed(0)\n",
    "test_size = 0.1\n",
    "num_data = len(squad_dataset['data'])\n",
    "test_data = random.sample(squad_dataset['data'], int(test_size * num_data))\n",
    "train_data = [d for d in squad_dataset['data'] if d not in test_data]\n",
    "\n",
    "# Evaluate BERT on ROUGE metrics\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(predicted_answers, test_data['answers'])\n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
